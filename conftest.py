import pytest
import re
from playwright.sync_api import sync_playwright, Playwright
import allure
import time
from collections import Counter, defaultdict, deque
import statistics
from typing import Dict, Any
import json
from pathlib import Path
import requests
import config

CHROMIUM_PATH = "/opt/chromium/chrome"

DEVICES = ["Desktop", "Mobile"]
THROTTLING_MODES = ["No_throttling", "Slow_4G"]
GEO_LOCATIONS = ["Moscow", "SPb", "Kazan", "Novosibirsk", "Yekaterinburg"]
BROWSERS = ["chromium", "firefox", "webkit"]
PAY_METHODS = ["card", 'sbp']


geo_map = {
    "Moscow": ("ru-RU", "Europe/Moscow"),
    "SPb": ("ru-RU", "Europe/Moscow"),
    "Kazan": ("ru-RU", "Europe/Moscow"),
    "Novosibirsk": ("ru-RU", "Asia/Novosibirsk"),
    "Yekaterinburg": ("ru-RU", "Asia/Yekaterinburg"),
}

def pytest_addoption(parser):
    parser.addoption(
        '--film-url',
        action='store',
        default="https://calls7.com/movie/370",
        help="Choose url for film which page you want to test"
    )
    parser.addoption(
        "--film-list",
        action="store",
        default=None,
        help="–ü—É—Ç—å –∫ films.json –∏–ª–∏ films.txt —Å–æ —Å–ø–∏—Å–∫–æ–º URL"
    )
    parser.addoption(
        "--film-limit",
        action="store",
        type=int,
        default=None,
        help="Number of urls from list"
    )
    parser.addoption(
        '--device',
        action='store',
        default="Desktop",
        choices=DEVICES
    )
    parser.addoption(
        "--throttling",
        action="store",
        default="No_throttling",
        choices=THROTTLING_MODES
    )
    parser.addoption(
        "--geo",
        action="store",
        default="Moscow",
        choices=GEO_LOCATIONS
    )
    parser.addoption(
        "--browser",
        action="store",
        default="chromium",
        choices=BROWSERS
    )
    parser.addoption(
        "--pay-method",
        action="store",
        default="card",
        choices=PAY_METHODS    
    )

    
@pytest.fixture()
def get_film_url(request):
    return request.config.getoption("--film-url")

@pytest.fixture
def device(request):
    return request.config.getoption("--device")

@pytest.fixture
def throttling(request):
    return request.config.getoption("--throttling")

@pytest.fixture
def geo(request):
    return request.config.getoption("--geo")

@pytest.fixture(scope="session")
def browser_type(request):
    return request.config.getoption("--browser")

@pytest.fixture
def film_list(request):
    return request.config.getoption("--film-list")

@pytest.fixture
def film_limit(request):
    return request.config.getoption("--film-limit")

@pytest.fixture
def pay_method(request):
    return request.config.getoption("--pay-method")

def load_film_urls(film_list_path: str, limit: int = None) -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL –∏–∑ JSON –∏–ª–∏ TXT."""
    path = Path(film_list_path)
    if not path.exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {film_list_path}")

    if path.suffix == ".json":
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            urls = data.get("urls", data) if isinstance(data, dict) else data
    elif path.suffix == ".txt":
        with open(path, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]
    else:
        raise ValueError(f"–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ .json –∏ .txt, –ø–æ–ª—É—á–µ–Ω–æ: {path.suffix}")
    
    if limit is not None:
        urls = urls[:limit]

    return urls

def pytest_generate_tests(metafunc):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑—É–µ—Ç —Ç–µ—Å—Ç—ã, –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω—ã —Ñ–∏–∫—Å—Ç—É—Ä—ã."""
    device = metafunc.config.getoption("--device")
    throttling = metafunc.config.getoption("--throttling")
    geo = metafunc.config.getoption("--geo")
    browser = metafunc.config.getoption("--browser")
    payment_method = metafunc.config.getoption("--pay-method")
    use_cli = any([device, throttling, geo, browser, payment_method])
    if not use_cli:
        if "device" in metafunc.fixturenames:
            metafunc.parametrize("device", DEVICES, scope="function")
        if "throttling" in metafunc.fixturenames:
            metafunc.parametrize("throttling", THROTTLING_MODES, scope="function")
        if "geo" in metafunc.fixturenames:
            metafunc.parametrize("geo", GEO_LOCATIONS, scope="function")
        if "browser_type" in metafunc.fixturenames:
            metafunc.parametrize("browser_type", BROWSERS, scope="session")
        if "pay_method" in metafunc.fixturenames:
            metafunc.parametrize("pay_method", PAY_METHODS, scope="function")
            
    film_url = metafunc.config.getoption("--film-url")
    film_list = metafunc.config.getoption("--film-list")
    film_limit = metafunc.config.getoption("--film-limit")
    
    if "get_film_url" in metafunc.fixturenames:
        if film_list:
            urls = load_film_urls(film_list, limit=film_limit)
            metafunc.parametrize(
                "get_film_url",
                urls,
                scope="function",
                ids=lambda x: x.split("/")[-2]  # —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ ID: kvest, chernyy-zamok
            )
        elif film_url:
            metafunc.parametrize("get_film_url", [film_url], scope="function")
        else:
            # –ù–µ—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî –æ–¥–∏–Ω –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–π —Ç–µ—Å—Ç
            metafunc.parametrize("get_film_url", [None], scope="function")


@pytest.fixture(scope="session")
def playwright_instance():
    with sync_playwright() as p:
        yield p

@pytest.fixture(scope="session")
def browser_instance(playwright_instance, browser_type):
    p = playwright_instance
    if browser_type == "chromium":
            browser = p.chromium.launch(
                headless=True,
                executable_path=CHROMIUM_PATH,
                args=[
                    "--no-sandbox",
                    #"--remote-debugging-port=9222",
                    "--disable-gpu",
                    "--disable-dev-shm-usage"
                ],
            )
    elif browser_type == "firefox":
        browser = p.firefox.launch(headless=True)
    elif browser_type == "webkit":
        browser = p.webkit.launch(headless=True)
    else:
        raise ValueError(f"Unsupported: {browser}")
    yield browser
    browser.close()

@pytest.fixture(scope='function')
def page(browser_type, device, geo, throttling, browser_instance, playwright_instance):
    p = playwright_instance
    context_args = {}
        
    if device == "Mobile":
        p_config = dict(p.devices["Pixel 5"])
        if browser_type != "chromium":
            p_config.pop("is_mobile", None)
            p_config.pop("has_touch", None)
        context_args = p_config
    else:
        context_args["viewport"] = {"width": 1920, "height": 1080}
            
    locale, timezone = geo_map.get(geo, ("ru-RU", "UTC"))
    context_args.update({
        "locale": locale,
        "timezone_id": timezone,
    })

    context_args.update({
        "user_agent": (
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36"
        ),
        "permissions": ["geolocation", "notifications"],
        "java_script_enabled": True,
    })
        
    context = browser_instance.new_context(**context_args)
    # —Å–∫—Ä—ã–≤–∞–µ–º navigator.webdriver
    context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
            // –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–æ–Ω—Å–æ–ª–∏
            (function() {
                const originalConsole = {
                    log: console.log,
                    info: console.info,
                    debug: console.debug,
                    warn: console.warn,
                    error: console.error
                };
            
                function interceptConsole(method, args) {
                    try {
                        const message = args.map(arg => {
                            if (arg === null) return 'null';
                            if (arg === undefined) return 'undefined';
                            if (typeof arg === 'object') {
                                try {
                                    return JSON.stringify(arg);
                                } catch(e) {
                                    return String(arg);
                                }
                            }
                            return String(arg);
                        }).join(' ');
                        
                        // –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
                        if (!window.__consoleMessages) {
                            window.__consoleMessages = [];
                        }
                        window.__consoleMessages.push({
                            type: method,
                            message: message,
                            timestamp: Date.now()
                        });
                        
                        // –û—Ç–º–µ—á–∞–µ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –ø–ª–µ–µ—Ä–∞
                        if (message.includes('loadPlayer finished')) {
                            window.__playerReadyDetected = true;
                            window.__playerReadyTimestamp = Date.now();
                            console.log('[MONITOR] üéØ Player ready detected!');
                        }
                        
                        // –í—ã–∑—ã–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
                        originalConsole[method].apply(console, args);
                    } catch(e) {
                        originalConsole[method].apply(console, args);
                    }
                }
            
                // –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã console
                ['log', 'info', 'debug', 'warn', 'error'].forEach(method => {
                    console[method] = function(...args) {
                        interceptConsole(method, args);
                    };
                });
            })();
        """)
    context.clear_cookies()
    page = context.new_page()
    
    if browser_type == "chromium":
        client = context.new_cdp_session(page)
        client.send("Runtime.enable")
        client.send("Log.enable")
        
        def on_log_entry(params):
            text = params.get("entry", {}).get("text", "")
                # –ò–ª–∏ –∏–∑ args, –µ—Å–ª–∏ text –ø—É—Å—Ç–æ–π:
            args = params.get("entry", {}).get("args", [])
            if not text and args:
                text = " ".join(str(arg.get("value", "")) for arg in args)
                
            if "[Dc] loadPlayer finished" in text:
                page.evaluate("""
                    window.__playerReadyDetected = true;
                    window.__playerReadyTimestamp = Date.now();
                    window.__cdpDetected = true;
                """)
                print(f"[PLAYER] ‚úÖ [Dc] loadPlayer finished: {text}")
                
        client.on("Log.entryAdded", on_log_entry)
        time.sleep(0.1)
        if throttling == "Slow_4G":
            try:
                client.send("Network.enable")
                client.send("Network.emulateNetworkConditions", {
                    "offline": False,
                    "latency": 400,
                    "downloadThroughput": 700 * 1024,
                    "uploadThroughput": 700 * 1024,
                    "connectionType": "cellular4g"
                })
                # –î–∞—ë–º —Å–µ—Ç–∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å—Å—è
                time.sleep(0.5)
            except Exception as e:
                print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ç—Ä–æ—Ç—Ç–ª–∏–Ω–≥: {e}")

                
    yield page
    context.close()
    

@pytest.hookimpl(tryfirst=True, hookwrapper=True)
def pytest_runtest_makereport(item, call):
    outcome = yield
    rep = outcome.get_result()
    
    # 1. –°–∫—Ä–∏–Ω—à–æ—Ç –ø—Ä–∏ –ø–∞–¥–µ–Ω–∏–∏
    if rep.when == "call" and rep.failed:
        page = item.funcargs.get("page")
        if page:
            try:
                allure.attach(
                    page.screenshot(),
                    name="screenshot",
                    attachment_type=allure.attachment_type.PNG
                )
            except Exception as e:
                print(f"[WARN] –°–∫—Ä–∏–Ω—à–æ—Ç –Ω–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {e}")
    
    # 2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ report ‚Äî –¥–∞–∂–µ –µ—Å–ª–∏ —Ç–µ—Å—Ç —É–ø–∞–ª
    if rep.when == "call":
        if hasattr(item, "_report_data") and isinstance(item._report_data, dict):
            test_name = item.nodeid.split("::")[-1].split("[")[0]
            _aggregator.add_report(test_name, item._report_data)
            

class MultiTestRunAggregator:
    def __init__(self):
        self.reports_by_test = defaultdict(list)
        self.cluster_cache = {}
    
    def add_report(self, test_name: str, report: dict):
        self.reports_by_test[test_name].append(report)
        if test_name in self.cluster_cache:
            del self.cluster_cache[test_name]
        
    def step_factory(self):
        return {"metrics": defaultdict(list), "booleans": defaultdict(list)}
    
    def get_summary(self, test_name: str) -> dict:
        reports = self.reports_by_test[test_name]
        if not reports:
            return {"error": f"No reports for {test_name}"}
        
        summary = {
            "test_name": test_name,
            "domain": reports[0]["domain"],
            "total_runs": len(reports),
            "problematic_runs": sum(1 for r in reports if r.get("is_problematic_flow", False)),
            "failed_runs": sum(1 for r in reports if r.get("error")),
            "steps": defaultdict(self.step_factory),
            "distribution": {
                "device": defaultdict(int),
                "throttling": defaultdict(int),
                "geo": defaultdict(int),
                "browser": defaultdict(int),
            },
            "film_urls": set(),
            "errors": defaultdict(list),
        }

        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        for r in reports:
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            summary["distribution"]["device"][r.get("device", "N/A")] += 1
            summary["distribution"]["throttling"][r.get("throttling", "N/A")] += 1
            summary["distribution"]["geo"][r.get("geoposition", "N/A")] += 1
            summary["distribution"]["browser"][r.get("browser_type", "N/A")] += 1
            summary["film_urls"].add(r.get("film_url", "").strip())
            
            error_msg = r.get("error")
            if error_msg:
                # –£–ø—Ä–æ—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ: –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Ç–∏–ø –∏ –ø–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤
                simplified = re.sub(r"Call log:.*", "", error_msg).strip()
                simplified = re.sub(r"\s+", " ", simplified)[:100]
                summary["errors"][simplified].append(r)

            # –°–±–æ—Ä –í–°–ï–• –º–µ—Ç—Ä–∏–∫ –ø–æ —à–∞–≥–∞–º
            for step_name, metrics in r.get("steps", {}).items():
                if not isinstance(metrics, dict):
                    continue
                
                if step_name not in summary["steps"]:
                    summary["steps"][step_name] = self.step_factory()
                
                for metric_name, value in metrics.items():
                    if value is None:
                        continue
                
                    if isinstance(value, bool):
                        # –ë—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
                        summary["steps"][step_name]["booleans"][metric_name].append(value)
                    elif isinstance(value, (int, float)):
                        # –ß–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                        summary["steps"][step_name]["metrics"][metric_name].append(value)

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        for step_name, step_data in summary["steps"].items():
            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
            for metric_name, values in step_data["metrics"].items():
                if values:
                    try:
                        step_data["metrics"][metric_name] = {
                            "mean": round(statistics.mean(values), 1),
                            "median": round(statistics.median(values), 1),
                            "min": min(values),
                            "max": max(values),
                            "count": len(values),
                            "stdev": round(statistics.stdev(values), 1) if len(values) > 1 else 0.0,
                        }
                    except statistics.StatisticsError:
                        step_data["metrics"][metric_name] = {"values": values}

            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –±—É–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
            for metric_name, values in step_data["booleans"].items():
                if values:
                    true_count = sum(values)
                    total = len(values)
                    step_data["booleans"][metric_name] = {
                        "true_count": true_count,
                        "false_count": total - true_count,
                        "true_percentage": round(true_count / total * 100, 1) if total > 0 else 0,
                        "total": total
                    }


        # –û—Ç–¥–µ–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è PPI
        for step_name, step_data in summary["steps"].items():
            ppi_values = []
            for r in reports:
                step_metrics = r.get("steps", {}).get(step_name, {})
                if isinstance(step_metrics, dict):
                    ppi = step_metrics.get("pagePerformanceIndex")
                    if isinstance(ppi, (int, float)) and ppi is not None:
                        ppi_values.append(ppi)
            
            if ppi_values:
                step_data["ppi_stats"] = {
                    "mean": round(statistics.mean(ppi_values), 1),
                    "median": round(statistics.median(ppi_values), 1),
                    "min": min(ppi_values),
                    "max": max(ppi_values),
                    "stdev": round(statistics.stdev(ppi_values), 1) if len(ppi_values) > 1 else 0.0,
                }

        summary["film_urls"] = list(summary["film_urls"])
        return summary
    
    def save_summary(self, test_name: str):
        summary = self.get_summary(test_name)
        print(f"[DEBUG] save_summary({test_name}) ‚Üí keys: {list(summary.keys())}")
        if "error" in summary:
            print(f"[INFO] –ü—Ä–æ–ø—É—â–µ–Ω –∞–≥—Ä–µ–≥–∞—Ç –¥–ª—è '{test_name}': {summary['error']}")
            return
    
        reports_dir = Path("reports")
        reports_dir.mkdir(exist_ok=True)

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
        safe_name = re.sub(r'[<>:"/\\|?*\s]', '_', test_name)[:30]
        json_path = reports_dir / f"RUN_SUMMARY_{safe_name}_{summary['domain']}.json"
        md_path = reports_dir / f"RUN_SUMMARY_{safe_name}_{summary['domain']}.md"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º MD
        self._save_markdown(summary, md_path)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
        try:
            self.save_clustered_summaries(test_name)
        except Exception as e:
            print(f"[WARNING] Failed to save clustered summaries: {e}")
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        try:
            self.create_cluster_comparison_report(test_name)
        except Exception as e:
            print(f"[WARNING] Failed to create cluster comparison: {e}")

    def _save_markdown(self, summary: dict, path: Path):
        md_lines = []
        test_name = summary.get("test_name", "unknown")
        total = summary.get("total_runs", 0)
        problematic = summary.get("problematic_runs", 0)
        failed = summary["failed_runs"]
        
        md_lines.append(f"# üìä –ò—Ç–æ–≥ –ø–æ —Ç–µ—Å—Ç—É: `{test_name}`\n")
        md_lines.append(f"**–î–∞—Ç–∞**: `{time.strftime('%Y-%m-%d %H:%M:%S')}`")
        md_lines.append(f"**–í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤**: `{total}`")
        md_lines.append(f"**–ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö (–ø–æ –º–µ—Ç—Ä–∏–∫–∞–º)**: `{problematic}` (`{problematic/total*100:.1f}%`)")
        md_lines.append(f"**–£–ø–∞–≤—à–∏—Ö (–ø–æ –æ—à–∏–±–∫–∞–º)**: `{failed}` (`{failed/total*100:.1f}%`)")
        md_lines.append("")
        
        if summary["errors"]:
            md_lines.append("## üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏")
            md_lines.append("| –û—à–∏–±–∫–∞ | –ß–∞—Å—Ç–æ—Ç–∞ | –ü—Ä–∏–º–µ—Ä URL |")
            md_lines.append("|--------|---------|------------|")
            for error_msg, reports in sorted(summary["errors"].items(), key=lambda x: len(x[1]), reverse=True):
                count = len(reports)
                pct = count / total * 100
                example_url = reports[0].get("film_url", "N/A").split("?")[0]
                md_lines.append(f"| `{error_msg}` | `{count}` (`{pct:.1f}%`) | `{example_url}` |")
            md_lines.append("")

        # –§–∏–ª—å–º—ã
        films = summary["film_urls"]
        md_lines.append("### üé¨ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã")
        for url in films[:5]:
            md_lines.append(f"- `{url}`")
        if len(films) > 5:
            md_lines.append(f"- ... –∏ –µ—â—ë {len(films) - 5}")
        md_lines.append("")

        # –°–≤–æ–¥–∫–∞ –ø–æ —à–∞–≥–∞–º
        md_lines.append("### üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —à–∞–≥–∞–º")
        md_lines.append("| –®–∞–≥ | –°—Ä–µ–¥–Ω–∏–π PPI | –í–∞—Ä–∏–∞—Ü–∏—è (œÉ) | –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ |")
        md_lines.append("|-----|-------------|--------------|-------------------|")

        for step_name, data in summary["steps"].items():
            ppi_stats = data.get("ppi_stats", {})
            ppi_mean = ppi_stats.get("mean", "‚Äî")
            ppi_stdev = ppi_stats.get("stdev", "‚Äî")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —à–∞–≥–∞
            key_metrics = []
            metrics_data = data.get("metrics", {})
            
            # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —à–∞–≥–∞
            if step_name == "main_page":
                for metric in ["lcp", "fcp", "cls"]:
                    if metric in metrics_data:
                        val = metrics_data[metric].get("mean", "‚Äî")
                        key_metrics.append(f"{metric.upper()}: {val}")
            elif step_name == "film_page":
                for metric in ["videoStartTime", "playerInitTime", "lcp"]:
                    if metric in metrics_data:
                        val = metrics_data[metric].get("mean", "‚Äî")
                        key_metrics.append(f"{metric}: {val}")
            elif step_name == "pay_page":
                for metric in ["iframeCpLoadTime"]:
                    if metric in metrics_data:
                        val = metrics_data[metric].get("mean", "‚Äî")
                        key_metrics.append(f"{metric}: {val}")
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ—Å—Ç—É–ø–Ω—ã–µ
            if not key_metrics:
                available_metrics = list(metrics_data.keys())[:3]
                for metric in available_metrics:
                    if not metric.endswith('_count'):
                        val = metrics_data[metric].get("mean", "‚Äî")
                        unit = self._get_metric_unit(metric)
                        key_metrics.append(f"{metric}: {val}{unit}")
            
            key_metrics_str = ", ".join(key_metrics) if key_metrics else "‚Äî"
            md_lines.append(f"| `{step_name}` | `{ppi_mean}` | `{ppi_stdev}` | `{key_metrics_str}` |")
        md_lines.append("")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        md_lines.append("### üåç –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º")
        for dim, counts in summary["distribution"].items():
            md_lines.append(f"#### `{dim}`")
            md_lines.append("| –ó–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
            md_lines.append("|----------|------------|")
            for val, cnt in sorted(counts.items()):
                md_lines.append(f"| `{val}` | `{cnt}` |")
            md_lines.append("")

        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        problematic_metrics = self._analyze_problematic_metrics(summary)

        if problematic_metrics:
            md_lines.append("### ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
            md_lines.extend(problematic_metrics)
            md_lines.append("")
            
        if failed > 0:
            md_lines.append("### üö® –£–ø–∞–≤—à–∏–µ —Ç–µ—Å—Ç—ã (–æ—à–∏–±–∫–∏)")
            md_lines.append(f"- –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ `{failed}` –ø–∞–¥–µ–Ω–∏–π (—Å–º. —Ä–∞–∑–¥–µ–ª ¬´–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏¬ª –≤—ã—à–µ)")
            md_lines.append("")
            
        if not (problematic_metrics or failed):
            md_lines.append("### ‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ\n")
            
        # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —à–∞–≥–∞–º
        md_lines.append("## üìã –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —à–∞–≥–∞–º")
        for step_name, step_data in summary["steps"].items():
            if not step_data.get("metrics"):
                continue

            title_map = {
                "main_page": "–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞",
                "film_page": "–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ñ–∏–ª—å–º–æ–º",
                "pay_page": "–û–ø–ª–∞—Ç–∞",
                "after_payment_popup": "–ü–æ–ø–∞–ø –ø–æ—Å–ª–µ –æ–ø–ª–∞—Ç—ã",
                "after_return_without_payment": "–í–æ–∑–≤—Ä–∞—Ç –±–µ–∑ –æ–ø–ª–∞—Ç—ã"
            }
            md_lines.append(f"\n### > {title_map.get(step_name, step_name)}:")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏: —Å–Ω–∞—á–∞–ª–∞ —á–∏—Å–ª–æ–≤—ã–µ, –ø–æ—Ç–æ–º –±—É–ª–µ–≤—ã
            metrics_data = step_data.get("metrics", {})
            for metric_name, stats in metrics_data.items():
                if not isinstance(stats, dict) or "mean" not in stats:
                    continue

                mean_val = stats["mean"]
                unit = self._get_metric_unit(metric_name)
                
                # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫–∏
                grade = config.grade_metric(mean_val, metric_name)
                icon = {"–æ—Ç–ª–∏—á–Ω–æ": "‚úÖ", "—Ö–æ—Ä–æ—à–æ": "üü¢", "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ": "üü°", "–ø–ª–æ—Ö–æ": "üî¥"}.get(grade, "‚ùì")

                # –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è
                nice_name = self._get_metric_display_name(metric_name)

                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–µ—Ç—Ä–∏–∫–∏
                if unit == "–º—Å":
                    value_str = f"{int(mean_val)} {unit}"
                    range_str = f"(min: {int(stats.get('min', 0))}, max: {int(stats.get('max', 0))})"
                elif unit == "":
                    value_str = f"{mean_val}"
                    range_str = f"(min: {stats.get('min', 0)}, max: {stats.get('max', 0)})"
                else:
                    value_str = f"{mean_val} {unit}"
                    range_str = f"(min: {stats.get('min', 0)}, max: {stats.get('max', 0)})"

                md_lines.append(f"{icon} **{nice_name}**: {value_str} {range_str} ‚Äî **{grade}**")

            # –í—ã–≤–æ–¥–∏–º –±—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏ (—Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –∏–º–µ—é—Ç –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞)
            booleans_data = step_data.get("booleans", {})
            for metric_name, stats in booleans_data.items():
                if not isinstance(stats, dict):
                    continue
                    
                true_percentage = stats.get('true_percentage', 0)
                total = stats.get('total', 0)
                
                # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –±—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏
                if true_percentage < 90:  # –ü–æ—Ä–æ–≥ –¥–ª—è –≤—ã–≤–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                    nice_name = self._get_metric_display_name(metric_name)
                    status_icon = "üî¥" if true_percentage < 70 else "üü°"
                    md_lines.append(
                        f"{status_icon} **{nice_name}**: {true_percentage}% —É—Å–ø–µ—à–Ω–æ "
                        f"({stats.get('true_count', 0)}/{total})"
                    )
                
        md_lines.append("")
            
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))
            
        allure.attach.file(
            path,
            name="–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —à–∞–≥–∞–º",
            extension="md"
        )

    def _analyze_problematic_metrics(self, summary: dict) -> list:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º"""
        problematic = []
        
        for step_name, data in summary["steps"].items():
            metrics_data = data.get("metrics", {})
            booleans_data = data.get("booleans", {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º PPI
            ppi_stats = data.get("ppi_stats", {})
            if ppi_stats.get("mean", 100) < config.TARGET_PAGE_PERFORMANCE_INDEX:
                problematic.append(f"- `{step_name}.pagePerformanceIndex`: {ppi_stats['mean']:.1f} < {config.TARGET_PAGE_PERFORMANCE_INDEX}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ—Ä–æ–≥–∞–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            for metric_name in config.METRIC_THRESHOLDS.keys():
                if metric_name in metrics_data:
                    mean_val = metrics_data[metric_name].get("mean", 0)
                    poor_threshold = config.METRIC_THRESHOLDS[metric_name][1]
                    if mean_val > poor_threshold:
                        unit = self._get_metric_unit(metric_name)
                        problematic.append(f"- `{step_name}.{metric_name}`: {mean_val:.0f}{unit} > {poor_threshold}{unit}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏
            for metric_name, stats in booleans_data.items():
                if not isinstance(stats, dict):
                    continue
                    
                true_percentage = stats.get('true_percentage', 100)
                if true_percentage < 90:  # –ü–æ—Ä–æ–≥ –¥–ª—è –±—É–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
                    problematic.append(f"- `{step_name}.{metric_name}`: {true_percentage}% —É—Å–ø–µ—à–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–π < 90%")
        
        return problematic
    
    def _get_metric_unit(self, metric_name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–¥–∏–Ω–∏—Ü—É –∏–∑–º–µ—Ä–µ–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏"""
        units = {
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã)
            "videoStartTime": "–º—Å",
            "playerInitTime": "–º—Å",
            "popupAppearTime": "–º—Å",
            "iframeCpLoadTime": "–º—Å",
            "lcp": "–º—Å",
            "ttfb": "–º—Å",
            "fcp": "–º—Å",
            "tbt": "–º—Å",
            "inp": "–º—Å",
            "dnsResolveTime": "–º—Å",
            "connectTime": "–º—Å",
            "rebufferDuration": "–º—Å",
            "viduPopupAppearTime": "–º—Å",
            "retryPaymentLoadTime": "–º—Å",
            
            # –ë–µ–∑—Ä–∞–∑–º–µ—Ä–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            "cls": "",
            "performance_score": "",
            "pagePerformanceIndex": "",
            "rebufferCount": "",
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            "true_percentage": "%",
        }
        return units.get(metric_name, "–º—Å")
    
    def _get_metric_display_name(self, metric_name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏"""
        names = {
            "videoStartTime": "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞ –≤–∏–¥–µ–æ",
            "playerInitTime": "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–ª–µ–µ—Ä–∞",
            "popupAppearTime": "–ü–æ—è–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏",
            "iframeCpLoadTime": "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ä–º—ã –æ–ø–ª–∞—Ç—ã",
            "lcp": "Largest Contentful Paint",
            "ttfb": "Time to First Byte",
            "fcp": "First Contentful Paint",
            "cls": "Cumulative Layout Shift",
            "tbt": "Total Blocking Time",
            "performance_score": "Performance Score",
            "pagePerformanceIndex": "Page Performance Index",
            "dnsResolveTime": "DNS Resolve Time",
            "connectTime": "Connect Time",
            "rebufferCount": "Rebuffer Count",
            "rebufferDuration": "Rebuffer Duration",
            "popupAvailable": "–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–æ–ø–∞–ø–∞",
            "popupClickSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∫–ª–∏–∫–∞ –ø–æ –ø–æ–ø–∞–ø—É",
            "buttonsCpAvailable": "–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–Ω–æ–ø–æ–∫ –æ–ø–ª–∞—Ç—ã",
            "buttonsClickSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∫–ª–∏–∫–∞ –ø–æ –∫–Ω–æ–ø–∫–∞–º",
            "payFormAppear": "–ü–æ—è–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –æ–ø–ª–∞—Ç—ã",
            "viduPopupSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ–ø–∞–ø–∞ Vidu",
            "retryPaymentSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–ø–ª–∞—Ç—ã",
            "is_problematic_page": "–ü—Ä–æ–±–ª–µ–º–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞",
        }
        return names.get(metric_name, metric_name)
    
    def get_clustered_summaries(self, test_name: str, cluster_by: list = None) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫–∏, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º"""
        if cluster_by is None:
            cluster_by = ["device", "throttling", "geoposition", "browser_type"]
        
        cache_key = f"{test_name}_{'_'.join(sorted(cluster_by))}"
        if cache_key in self.cluster_cache:
            return self.cluster_cache[cache_key]
        
        reports = self.reports_by_test[test_name]
        if not reports:
            return {"error": f"No reports for {test_name}"}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        clusters = defaultdict(list)
        for report in reports:
            cluster_key = tuple(report.get(param, "N/A") for param in cluster_by)
            clusters[cluster_key].append(report)
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        clustered_summaries = {}
        for cluster_key, cluster_reports in clusters.items():
            cluster_name_parts = []
            for param, value in zip(cluster_by, cluster_key):
                cluster_name_parts.append(f"{param}: {value}")
            cluster_name = "; ".join(cluster_name_parts)
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            temp_aggregator = MultiTestRunAggregator()
            for report in cluster_reports:
                temp_aggregator.add_report(test_name, report)
            
            clustered_summaries[cluster_name] = temp_aggregator.get_summary(test_name)
        
        self.cluster_cache[cache_key] = clustered_summaries
        return clustered_summaries
    
    def save_clustered_summaries(self, test_name: str, cluster_by: list = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã"""
        clustered_summaries = self.get_clustered_summaries(test_name, cluster_by)
        
        if "error" in clustered_summaries:
            print(f"[INFO] –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è '{test_name}': {clustered_summaries['error']}")
            return
        
        reports_dir = Path("reports") / "clustered"
        reports_dir.mkdir(exist_ok=True, parents=True)
        
        safe_name = re.sub(r'[<>:"/\\|?*\s]', '_', test_name)[:30]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
        for cluster_name, summary in clustered_summaries.items():
            safe_cluster_name = re.sub(r'[<>:"/\\|?*\s]', '_', cluster_name)[:50]
            
            json_path = reports_dir / f"CLUSTER_{safe_name}_{safe_cluster_name}.json"
            md_path = reports_dir / f"CLUSTER_{safe_name}_{safe_cluster_name}.md"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º MD —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            self._save_clustered_markdown(summary, cluster_name, md_path)
    
    def _save_clustered_markdown(self, summary: dict, cluster_name: str, path: Path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç Markdown –æ—Ç—á–µ—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        md_lines = []
        short_cluster_name = self._shorten_cluster_name(cluster_name)
        
        md_lines.append(f"# üéØ –ö–ª–∞—Å—Ç–µ—Ä: `{cluster_name}`\n")
        md_lines.append(f"**–¢–µ—Å—Ç**: `{summary.get('test_name', 'unknown')}`")
        md_lines.append(f"**–î–æ–º–µ–Ω**: `{summary.get('domain', 'unknown')}`")
        md_lines.append(f"**–î–∞—Ç–∞**: `{time.strftime('%Y-%m-%d %H:%M:%S')}`")
        md_lines.append(f"**–ó–∞–ø—É—Å–∫–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ**: `{summary.get('total_runs', 0)}`\n")
        
        # –û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞ –æ–±—ã—á–Ω–æ–º—É –æ—Ç—á–µ—Ç—É, –Ω–æ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        self._add_common_markdown_content(summary, md_lines)
        
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))
    
    def _shorten_cluster_name(self, cluster_name: str) -> str:
        """–°–æ–∫—Ä–∞—â–∞–µ—Ç –∏–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö"""
        # –ó–∞–º–µ–Ω—è–µ–º –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ
        """–ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ (—Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è)"""
        parts = cluster_name.split("; ")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è
        values = []
        for part in parts:
            if ": " in part:
                value = part.split(": ")[1]
                # –°–æ–∫—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                short_value = {
                    "Desktop": "DT",
                    "No_throttling": "NoThrot",
                    "firefox": "FF",
                    "webkit": "WK",
                    "chromium": "CH", 
                    "Moscow": "MSK",
                    "Novosibirsk": "NSK",
                    "N/A": "NA"
                }.get(value, value[:3])
                values.append(short_value)
        
        return "-".join(values)

    def _add_common_markdown_content(self, summary: dict, md_lines: list):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—â–µ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ Markdown (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –æ–±—ã—á–Ω—ã—Ö –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞—Ö)"""
        total = summary.get("total_runs", 0)
        problematic = summary.get("problematic_runs", 0)
        failed = summary.get("failed_runs", 0)
        
        md_lines.append(f"**–ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤**: `{problematic}` (`{problematic/total*100:.1f}%`)")
        md_lines.append(f"**–£–ø–∞–≤—à–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤**: `{failed}` (`{failed/total*100:.1f}%`)\n")
        
        if summary.get("errors"):
            md_lines.append("## üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏")
            md_lines.append("| –û—à–∏–±–∫–∞ | –ß–∞—Å—Ç–æ—Ç–∞ | –ü—Ä–∏–º–µ—Ä URL |")
            md_lines.append("|--------|---------|------------|")
            for error_msg, reports in sorted(summary["errors"].items(), key=lambda x: len(x[1]), reverse=True):
                count = len(reports)
                pct = count / total * 100
                example_url = reports[0].get("film_url", "N/A").split("?")[0]
                md_lines.append(f"| `{error_msg}` | `{count}` (`{pct:.1f}%`) | `{example_url}` |")
            md_lines.append("")

        # –°–≤–æ–¥–∫–∞ –ø–æ —à–∞–≥–∞–º
        md_lines.append("### üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —à–∞–≥–∞–º")
        md_lines.append("| –®–∞–≥ | –°—Ä–µ–¥–Ω–∏–π PPI | –ú–µ–¥–∏–∞–Ω–∞ PPI | –í–∞—Ä–∏–∞—Ü–∏—è (œÉ) |")
        md_lines.append("|-----|-------------|-------------|--------------|")

        for step_name, data in summary["steps"].items():
            ppi_stats = data.get("ppi_stats", {})
            ppi_mean = ppi_stats.get("mean", "‚Äî")
            ppi_median = ppi_stats.get("median", "‚Äî")
            ppi_stdev = ppi_stats.get("stdev", "‚Äî")
            
            md_lines.append(f"| `{step_name}` | `{ppi_mean}` | `{ppi_median}` | `{ppi_stdev}` |")
        md_lines.append("")

        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        problematic_metrics = self._analyze_problematic_metrics(summary)
        if problematic_metrics:
            md_lines.append("### ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
            md_lines.extend(problematic_metrics)
            md_lines.append("")
            
    def create_cluster_comparison_report(self, test_name: str, cluster_by: list = None):
        """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç —Å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        try:
            clustered_summaries = self.get_clustered_summaries(test_name, cluster_by)
            
            if "error" in clustered_summaries:
                print(f"[INFO] Cannot create cluster comparison: {clustered_summaries['error']}")
                return
            
            reports_dir = Path("reports")
            safe_name = re.sub(r'[<>:"/\\|?*\s]', '_', test_name)[:30]
            md_path = reports_dir / f"CLUSTER_COMPARISON_{safe_name}.md"
            
            md_lines = []
            md_lines.append(f"# üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: `{test_name}`\n")
            md_lines.append(f"**–î–∞—Ç–∞**: `{time.strftime('%Y-%m-%d %H:%M:%S')}`")
            md_lines.append(f"**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏**: `{', '.join(cluster_by) if cluster_by else 'device, throttling, geo, browser_type'}`")
            md_lines.append(f"**–í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤**: `{len(clustered_summaries)}`\n")
            
            # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
            md_lines.append("## üìà –°–≤–æ–¥–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")
            md_lines.append("| –ö–ª–∞—Å—Ç–µ—Ä | –ó–∞–ø. | –ü—Ä–æ–±–ª. | –£–ø–∞–ª–æ | PPI |")
            md_lines.append("|---------|------|---------|-------|-----|")
            
            for cluster_name, summary in clustered_summaries.items():
                total = summary.get("total_runs", 0)
                problematic = summary.get("problematic_runs", 0)
                failed = summary.get("failed_runs", 0)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π PPI –ø–æ –≤—Å–µ–º —à–∞–≥–∞–º
                avg_ppi = self._calculate_average_ppi(summary)
                
                short_name = self._shorten_cluster_name(cluster_name)
                
                md_lines.append(
                    f"| `{short_name}` | `{total}` | `{problematic}` | `{failed}` | `{avg_ppi:.1f}` |"
                )
            
            md_lines.append("\n")
            
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
            md_lines.append("## üîç –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫")
            
            # –î–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∂–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            important_metrics = [
                ("film_page", "videoStartTime", "–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ"),
                ("film_page", "popupAppearTime", "–ü–æ—è–≤–ª–µ–Ω–∏–µ –ø–æ–ø–∞–ø–∞"), 
                ("pay_page", "iframeCpLoadTime", "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ä–º—ã –æ–ø–ª–∞—Ç—ã"),
                ("film_page", "lcp", "LCP"),
                ("film_page", "pagePerformanceIndex", "PPI")
            ]
            
            for step, metric_name, display_name in important_metrics:
                md_lines.append(f"### üìä {display_name}")
                md_lines.append("| –ö–ª–∞—Å—Ç–µ—Ä | –°—Ä–µ–¥–Ω–µ–µ | –ú–µ–¥–∏–∞–Ω–∞ | Min | Max | –°—Ç–∞—Ç—É—Å |")
                md_lines.append("|---------|---------|---------|-----|-----|--------|")
                
                for cluster_name, summary in clustered_summaries.items():
                    step_data = summary.get("steps", {}).get(step, {})
                    metrics_data = step_data.get("metrics", {}).get(metric_name, {})
                    
                    if not metrics_data or "mean" not in metrics_data:
                        short_name = self._shorten_cluster_name(cluster_name)
                        md_lines.append(f"| `{short_name}` | ‚Äî | ‚Äî | ‚Äî | ‚Äî | ‚ùì |")
                        continue
                    
                    mean_val = metrics_data["mean"]
                    median_val = metrics_data["median"]
                    min_val = metrics_data["min"]
                    max_val = metrics_data["max"]
                    
                    # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫–∏
                    grade = config.grade_metric(mean_val, metric_name)
                    icon = {"–æ—Ç–ª–∏—á–Ω–æ": "‚úÖ", "—Ö–æ—Ä–æ—à–æ": "üü¢", "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ": "üü°", "–ø–ª–æ—Ö–æ": "üî¥"}.get(grade, "‚ùì")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –µ–¥–∏–Ω–∏—Ü—É –∏–∑–º–µ—Ä–µ–Ω–∏—è –î–û –µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                    unit = self._get_metric_unit(metric_name)
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                    if unit == "–º—Å":
                        mean_str = f"{int(mean_val)}"
                        median_str = f"{int(median_val)}"
                        min_str = f"{int(min_val)}"
                        max_str = f"{int(max_val)}"
                    else:
                        mean_str = f"{mean_val:.1f}"
                        median_str = f"{median_val:.1f}"
                        min_str = f"{min_val:.1f}"
                        max_str = f"{max_val:.1f}"
                    
                    short_name = self._shorten_cluster_name(cluster_name)
                    md_lines.append(
                        f"| `{short_name}` | `{mean_str}{unit}` | `{median_str}{unit}` | "
                        f"`{min_str}{unit}` | `{max_str}{unit}` | {icon} |"
                    )
                
                md_lines.append("")
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            md_lines.append("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
            analysis_results = self._analyze_clusters_statistically(clustered_summaries)
            
            if analysis_results["anomalies"]:
                md_lines.append("### ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏")
                for anomaly in analysis_results["anomalies"]:
                    md_lines.append(f"- {anomaly}")
                md_lines.append("")
            
            if analysis_results["recommendations"]:
                md_lines.append("### üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
                for recommendation in analysis_results["recommendations"]:
                    md_lines.append(f"- {recommendation}")
                md_lines.append("")
            
            if analysis_results["best_performing"]:
                md_lines.append("### üèÜ –õ—É—á—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã")
                for best in analysis_results["best_performing"]:
                    md_lines.append(f"- {best}")
                md_lines.append("")
            
            if not any([analysis_results["anomalies"], analysis_results["recommendations"], analysis_results["best_performing"]]):
                md_lines.append("### ‚ÑπÔ∏è –û—Å–æ–±—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ\n")
            
            with open(md_path, "w", encoding="utf-8") as f:
                f.write("\n".join(md_lines))
            
            allure.attach.file(
                md_path,
                name="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤",
                extension="md"
            )
        except Exception as e:
            print(f"[ERROR] Failed to create cluster comparison report: {e}")
            import traceback
            traceback.print_exc()
    
    def _calculate_average_ppi(self, summary: dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–π PPI –ø–æ –≤—Å–µ–º —à–∞–≥–∞–º"""
        try:
            ppi_values = []
            for step_name, step_data in summary.get("steps", {}).items():
                ppi_stats = step_data.get("ppi_stats", {})
                if "mean" in ppi_stats:
                    ppi_mean = ppi_stats["mean"]
                    if isinstance(ppi_mean, (int, float)) and ppi_mean > 0:
                        ppi_values.append(ppi_mean)
        
            if ppi_values:
                return statistics.mean(ppi_values)
            else:
                return 0.0
        except Exception as e:
            print(f"[ERROR] Error calculating average PPI: {e}")
            return 0.0
    
    def _find_cluster_anomalies(self, clustered_summaries: dict) -> list:
        """–ù–∞—Ö–æ–¥–∏—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        anomalies = []
        
        # –ê–Ω–∞–ª–∏–∑ PPI –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
        ppi_values = []
        for cluster_name, summary in clustered_summaries.items():
            avg_ppi = self._calculate_average_ppi(summary)
            ppi_values.append((cluster_name, avg_ppi))
        
        if ppi_values:
            avg_ppi_all = statistics.mean(ppi for _, ppi in ppi_values)
            std_ppi_all = statistics.stdev(ppi for _, ppi in ppi_values) if len(ppi_values) > 1 else 0
            
            for cluster_name, ppi in ppi_values:
                if std_ppi_all > 0 and abs(ppi - avg_ppi_all) > 2 * std_ppi_all:
                    anomalies.append(
                        f"–ö–ª–∞—Å—Ç–µ—Ä `{cluster_name}` –∏–º–µ–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–π PPI: {ppi:.1f} "
                        f"(—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {avg_ppi_all:.1f})"
                    )
        
        return anomalies
    
    def _analyze_clusters_statistically(self, clustered_summaries: dict) -> dict:
        """–ü—Ä–æ–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        results = {
            "anomalies": [],
            "recommendations": [],
            "best_performing": []
        }
        
        if len(clustered_summaries) < 2:
            results["anomalies"].append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            return results
        
        # –ê–Ω–∞–ª–∏–∑ PPI –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
        ppi_data = []
        for cluster_name, summary in clustered_summaries.items():
            avg_ppi = self._calculate_average_ppi(summary)
            ppi_data.append((cluster_name, avg_ppi, summary.get("total_runs", 0)))
        
        if ppi_data:
            ppi_values = [ppi for _, ppi, _ in ppi_data]
            avg_ppi_all = statistics.mean(ppi_values)
            
            if len(ppi_values) > 1:
                std_ppi_all = statistics.stdev(ppi_values)
                
                # –ù–∞—Ö–æ–¥–∏–º –∞–Ω–æ–º–∞–ª–∏–∏ (–±–æ–ª–µ–µ 2 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)
                for cluster_name, ppi, runs in ppi_data:
                    if std_ppi_all > 0 and abs(ppi - avg_ppi_all) > 2 * std_ppi_all:
                        short_name = self._shorten_cluster_name(cluster_name)
                        results["anomalies"].append(
                            f"–ö–ª–∞—Å—Ç–µ—Ä `{short_name}` –∏–º–µ–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–π PPI: {ppi:.1f} "
                            f"(—Å—Ä–µ–¥–Ω–µ–µ: {avg_ppi_all:.1f} ¬± {std_ppi_all:.1f})"
                        )
            
            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –∏ —Ö—É–¥—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            best_ppi = max(ppi_values)
            worst_ppi = min(ppi_values)
            
            for cluster_name, ppi, runs in ppi_data:
                short_name = self._shorten_cluster_name(cluster_name)
                if ppi == best_ppi and runs >= 3:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                    results["best_performing"].append(
                        f"`{short_name}` - –ª—É—á—à–∏–π PPI: {ppi:.1f} (–∑–∞–ø—É—Å–∫–æ–≤: {runs})"
                    )
                elif ppi == worst_ppi and runs >= 3:
                    results["anomalies"].append(
                        f"`{short_name}` - —Ö—É–¥—à–∏–π PPI: {ppi:.1f} (–∑–∞–ø—É—Å–∫–æ–≤: {runs})"
                    )
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ
        video_times = []
        for cluster_name, summary in clustered_summaries.items():
            film_metrics = summary.get("steps", {}).get("film_page", {})
            video_time = film_metrics.get("metrics", {}).get("videoStartTime", {}).get("mean", 0)
            if video_time > 0:
                video_times.append((cluster_name, video_time))
        
        if video_times:
            avg_video_time = statistics.mean(time for _, time in video_times)
            best_video = min(video_times, key=lambda x: x[1])
            worst_video = max(video_times, key=lambda x: x[1])
            
            short_best = self._shorten_cluster_name(best_video[0])
            short_worst = self._shorten_cluster_name(worst_video[0])
            
            results["recommendations"].append(
                f"–õ—É—á—à–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ: `{short_best}` ({best_video[1]:.0f}–º—Å), "
                f"—Ö—É–¥—à–µ–µ: `{short_worst}` ({worst_video[1]:.0f}–º—Å)"
            )
        
        # –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success_rates = []
        for cluster_name, summary in clustered_summaries.items():
            total = summary.get("total_runs", 0)
            failed = summary.get("failed_runs", 0)
            if total > 0:
                success_rate = (total - failed) / total * 100
                success_rates.append((cluster_name, success_rate, total))
        
        if success_rates:
            worst_success = min(success_rates, key=lambda x: x[1])
            if worst_success[1] < 80 and worst_success[2] >= 3:  # –ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
                short_name = self._shorten_cluster_name(worst_success[0])
                results["anomalies"].append(
                    f"–ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ `{short_name}`: {worst_success[1]:.1f}% "
                    f"({worst_success[2]-failed} –∏–∑ {worst_success[2]})"
                )
        
        return results

            

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä (–Ω–∞ —Å–µ—Å—Å–∏—é)
_aggregator = MultiTestRunAggregator()


@pytest.fixture(scope="session")
def aggregate_run_summary():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ—Å–ª–µ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤."""
    yield _aggregator


_test_run_counts = defaultdict(int)
_test_total_expected = {}

def pytest_collection_finish(session):
    """–°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –±—É–¥–µ—Ç –≤—ã–∑–≤–∞–Ω –∫–∞–∂–¥—ã–π —Ç–µ—Å—Ç (–∏–∑-–∑–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏–∏)."""
    global _test_total_expected
    for item in session.items:
        test_name = item.originalname or item.name.split("[")[0]
        _test_total_expected[test_name] = _test_total_expected.get(test_name, 0) + 1
        
_start_time = None

def pytest_sessionstart(session):
    global _start_time
    _start_time = time.time()
    
def aggregate_reports() -> dict:
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å–≤–æ–¥–∫—É –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ environment.properties –¥–ª—è Allure."""
    reports_dir = Path("reports")
    if not reports_dir.exists():
        return

    # –°–æ–±–∏—Ä–∞–µ–º –í–°–ï JSON-–æ—Ç—á—ë—Ç—ã
    reports = []
    for report_file in reports_dir.glob("report_*.json"):
        try:
            with open(report_file, "r", encoding="utf-8") as f:
                reports.append(json.load(f))
        except Exception as e:
            print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {report_file}: {e}")

    if not reports:
        return

    total = len(reports)
    problematic = sum(1 for r in reports if r.get("is_problematic_flow"))
    failed = sum(1 for r in reports if r.get("error"))

    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: —á–µ–º –º–µ–Ω—å—à–µ –ø—Ä–æ–±–ª–µ–º ‚Äî —Ç–µ–º –≤—ã—à–µ –æ—Ü–µ–Ω–∫–∞
    quality_score = max(0, int((1 - problematic / total) * 100))

    # –°—á—ë—Ç—á–∏–∫ –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–æ–±–ª–µ–º
    video_slow = 0
    lcp_bad = 0
    iframe_slow = 0
    for r in reports:
        steps = r.get("steps", {})
        # film_page.videoStartTime > 15 —Å–µ–∫
        vst = steps.get("film_page", {}).get("videoStartTime")
        if vst and vst > 15000:
            video_slow += 1
        # main_page.LCP > 2500 –º—Å
        lcp = steps.get("main_page", {}).get("lcp")
        if lcp and lcp > 2500:
            lcp_bad += 1
        # pay_page.iframeCpLoadTime > 3 —Å–µ–∫
        iframe = steps.get("pay_page", {}).get("iframeCpLoadTime")
        if iframe and iframe > 3000:
            iframe_slow += 1

    # –§–æ—Ä–º–∏—Ä—É–µ–º environment.properties
    env = {
        "Start time": time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(_start_time)),
        "End time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "Duration": f"{(time.time() - _start_time):.1f} sec",
        "Pages": f"{total} / 19975",
        "Problematic pages": f"{problematic} ({problematic/total*100:.1f}%)",
        "Failed by errors": f"{failed} ({failed/total*100:.1f}%)",
        "Quality score": f"{quality_score}%",
        # –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã ‚Äî –∫—Ä–∞—Ç–∫–æ, –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
        "film_page.videoStartTime > 15 sec": f"{video_slow} ({video_slow/total*100:.1f}%)",
        "main_page.LCP > 2500 ms": f"{lcp_bad} ({lcp_bad/total*100:.1f}%)",
        "pay_page.iframeCpLoadTime > 3 sec": f"{iframe_slow} ({iframe_slow/total*100:.1f}%)",
    }
    return env
    

def pytest_sessionfinish(session, exitstatus):
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ environment.properties –¥–ª—è Allure
    env_path = Path("allure-results")
    env_path.mkdir(exist_ok=True)
        
    env = aggregate_reports()
    with open(env_path / "environment.properties", "w", encoding="utf-8") as f:
        for key, value in env.items():
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –∑–Ω–∞–∫–∏ = –∏ \ –≤ –∑–Ω–∞—á–µ–Ω–∏—è—Ö (Allure —Ç—Ä–µ–±—É–µ—Ç)
            value = str(value).replace("\\", "\\\\").replace("=", "\\=")
            f.write(f"{key} = {value}\n")
            
    print(f"\n‚úÖ Environment –¥–ª—è Allure –æ–±–Ω–æ–≤–ª—ë–Ω: {env_path}")


def pytest_runtest_logfinish(nodeid, location):
    """–í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–æ—Å–ª–µ –ö–ê–ñ–î–û–ì–û –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–∞."""
    global _test_run_counts

    test_name = nodeid.split("::")[-1].split("[")[0]
    _test_run_counts[test_name] += 1

    # –ï—Å–ª–∏ –≤—Å–µ –∑–∞–ø—É—Å–∫–∏ —Ç–µ—Å—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω—ã ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –∞–≥—Ä–µ–≥–∞—Ç
    if _test_run_counts[test_name] == _test_total_expected.get(test_name, 1):
        _aggregator.save_summary(test_name)
        
    _aggregator.save_clustered_summaries(test_name, ["device", "throttling"])
    _aggregator.save_clustered_summaries(test_name, ["geo", "browser_type"])
    _aggregator.save_clustered_summaries(test_name, ["device"])

def send_telegram_report(summary_text: str, chat_id: str, bot_token: str):
    url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
    data = {
        "chat_id": chat_id,
        "text": f"üé¨ –¢–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã\n\n{summary_text}",
        "parse_mode": "Markdown"
    }
    requests.post(url, data=data)