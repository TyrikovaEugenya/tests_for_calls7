import re
import time
import statistics
import json
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Any, Optional
import allure
import config

class MultiTestRunAggregator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤ —Ç–µ—Å—Ç–æ–≤.
    
    –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É –∑–∞–ø—É—Å–∫–æ–≤
    - –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º (—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –≥–µ–æ, –±—Ä–∞—É–∑–µ—Ä –∏ —Ç.–¥.)
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ –≤ JSON –∏ Markdown —Ñ–æ—Ä–º–∞—Ç–∞—Ö
    - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã—è–≤–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
    """
    
    def __init__(self):
        self.reports_by_test = defaultdict(list)
        self.cluster_cache = {}
    
    def add_report(self, test_name: str, report: dict):
        self.reports_by_test[test_name].append(report)
        if test_name in self.cluster_cache:
            del self.cluster_cache[test_name]
        
    def step_factory(self):
        return {"metrics": defaultdict(list), "booleans": defaultdict(list)}
    
    def get_summary(self, test_name: str) -> dict:
        reports = self.reports_by_test[test_name]
        if not reports:
            return {"error": f"No reports for {test_name}"}
        
        summary = {
            "test_name": test_name,
            "domain": reports[0]["domain"],
            "total_runs": len(reports),
            "problematic_runs": sum(1 for r in reports if r.get("is_problematic_flow", False)),
            "failed_runs": sum(1 for r in reports if r.get("error")),
            "steps": defaultdict(self.step_factory),
            "distribution": {
                "device": defaultdict(int),
                "throttling": defaultdict(int),
                "geo": defaultdict(int),
                "browser": defaultdict(int),
            },
            "film_urls": set(),
            "errors": defaultdict(list),
        }

        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        for r in reports:
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            summary["distribution"]["device"][r.get("device", "N/A")] += 1
            summary["distribution"]["throttling"][r.get("throttling", "N/A")] += 1
            summary["distribution"]["geo"][r.get("geoposition", "N/A")] += 1
            summary["distribution"]["browser"][r.get("browser_type", "N/A")] += 1
            summary["film_urls"].add(r.get("film_url", "").strip())
            
            error_msg = r.get("error")
            if error_msg:
                # –£–ø—Ä–æ—â–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ: –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Ç–∏–ø –∏ –ø–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤
                simplified = re.sub(r"Call log:.*", "", error_msg).strip()
                simplified = re.sub(r"\s+", " ", simplified)[:100]
                summary["errors"][simplified].append(r)

            # –°–±–æ—Ä –í–°–ï–• –º–µ—Ç—Ä–∏–∫ –ø–æ —à–∞–≥–∞–º
            for step_name, metrics in r.get("steps", {}).items():
                if not isinstance(metrics, dict):
                    continue
                
                if step_name not in summary["steps"]:
                    summary["steps"][step_name] = self.step_factory()
                
                for metric_name, value in metrics.items():
                    if value is None:
                        continue
                
                    if isinstance(value, bool):
                        # –ë—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
                        summary["steps"][step_name]["booleans"][metric_name].append(value)
                    elif isinstance(value, (int, float)):
                        # –ß–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                        summary["steps"][step_name]["metrics"][metric_name].append(value)

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
        for step_name, step_data in summary["steps"].items():
            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
            for metric_name, values in step_data["metrics"].items():
                if values:
                    try:
                        step_data["metrics"][metric_name] = {
                            "mean": round(statistics.mean(values), 1),
                            "median": round(statistics.median(values), 1),
                            "min": min(values),
                            "max": max(values),
                            "count": len(values),
                            "stdev": round(statistics.stdev(values), 1) if len(values) > 1 else 0.0,
                        }
                    except statistics.StatisticsError:
                        step_data["metrics"][metric_name] = {"values": values}

            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –±—É–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
            for metric_name, values in step_data["booleans"].items():
                if values:
                    true_count = sum(values)
                    total = len(values)
                    step_data["booleans"][metric_name] = {
                        "true_count": true_count,
                        "false_count": total - true_count,
                        "true_percentage": round(true_count / total * 100, 1) if total > 0 else 0,
                        "total": total
                    }


        # –û—Ç–¥–µ–ª—å–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è PPI
        for step_name, step_data in summary["steps"].items():
            ppi_values = []
            for r in reports:
                step_metrics = r.get("steps", {}).get(step_name, {})
                if isinstance(step_metrics, dict):
                    ppi = step_metrics.get("pagePerformanceIndex")
                    if isinstance(ppi, (int, float)) and ppi is not None:
                        ppi_values.append(ppi)
            
            if ppi_values:
                step_data["ppi_stats"] = {
                    "mean": round(statistics.mean(ppi_values), 1),
                    "median": round(statistics.median(ppi_values), 1),
                    "min": min(ppi_values),
                    "max": max(ppi_values),
                    "stdev": round(statistics.stdev(ppi_values), 1) if len(ppi_values) > 1 else 0.0,
                }

        summary["film_urls"] = list(summary["film_urls"])
        return summary
    
    def save_summary(self, test_name: str):
        summary = self.get_summary(test_name)
        print(f"[DEBUG] save_summary({test_name}) ‚Üí keys: {list(summary.keys())}")
        if "error" in summary:
            print(f"[INFO] –ü—Ä–æ–ø—É—â–µ–Ω –∞–≥—Ä–µ–≥–∞—Ç –¥–ª—è '{test_name}': {summary['error']}")
            return
    
        reports_dir = Path("reports")
        reports_dir.mkdir(exist_ok=True)

        # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤
        safe_name = re.sub(r'[<>:"/\\|?*\s]', '_', test_name)[:30]
        json_path = reports_dir / f"RUN_SUMMARY_{safe_name}_{summary['domain']}.json"
        md_path = reports_dir / f"RUN_SUMMARY_{safe_name}_{summary['domain']}.md"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º MD
        self._save_markdown(summary, md_path)
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
        try:
            self.save_clustered_summaries(test_name)
        except Exception as e:
            print(f"[WARNING] Failed to save clustered summaries: {e}")
        # –°–æ–∑–¥–∞–µ–º –æ—Ç—á–µ—Ç —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        try:
            self.create_cluster_comparison_report(test_name)
        except Exception as e:
            print(f"[WARNING] Failed to create cluster comparison: {e}")

    def _save_markdown(self, summary: dict, path: Path):
        md_lines = []
        test_name = summary.get("test_name", "unknown")
        total = summary.get("total_runs", 0)
        problematic = summary.get("problematic_runs", 0)
        failed = summary["failed_runs"]
        
        md_lines.append(f"# üìä –ò—Ç–æ–≥ –ø–æ —Ç–µ—Å—Ç—É: `{test_name}`\n")
        md_lines.append(f"**–î–∞—Ç–∞**: `{time.strftime('%Y-%m-%d %H:%M:%S')}`")
        md_lines.append(f"**–í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤**: `{total}`")
        md_lines.append(f"**–ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö (–ø–æ –º–µ—Ç—Ä–∏–∫–∞–º)**: `{problematic}` (`{problematic/total*100:.1f}%`)")
        md_lines.append(f"**–£–ø–∞–≤—à–∏—Ö (–ø–æ –æ—à–∏–±–∫–∞–º)**: `{failed}` (`{failed/total*100:.1f}%`)")
        md_lines.append("")
        
        if summary["errors"]:
            md_lines.append("## üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏")
            md_lines.append("| –û—à–∏–±–∫–∞ | –ß–∞—Å—Ç–æ—Ç–∞ | –ü—Ä–∏–º–µ—Ä URL |")
            md_lines.append("|--------|---------|------------|")
            for error_msg, reports in sorted(summary["errors"].items(), key=lambda x: len(x[1]), reverse=True):
                count = len(reports)
                pct = count / total * 100
                example_url = reports[0].get("film_url", "N/A").split("?")[0]
                md_lines.append(f"| `{error_msg}` | `{count}` (`{pct:.1f}%`) | `{example_url}` |")
            md_lines.append("")

        # –§–∏–ª—å–º—ã
        films = summary["film_urls"]
        md_lines.append("### üé¨ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∏–ª—å–º—ã")
        for url in films[:5]:
            md_lines.append(f"- `{url}`")
        if len(films) > 5:
            md_lines.append(f"- ... –∏ –µ—â—ë {len(films) - 5}")
        md_lines.append("")

        # –°–≤–æ–¥–∫–∞ –ø–æ —à–∞–≥–∞–º
        md_lines.append("### üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —à–∞–≥–∞–º")
        md_lines.append("| –®–∞–≥ | –°—Ä–µ–¥–Ω–∏–π PPI | –í–∞—Ä–∏–∞—Ü–∏—è (œÉ) | –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ |")
        md_lines.append("|-----|-------------|--------------|-------------------|")

        for step_name, data in summary["steps"].items():
            ppi_stats = data.get("ppi_stats", {})
            ppi_mean = ppi_stats.get("mean", "‚Äî")
            ppi_stdev = ppi_stats.get("stdev", "‚Äî")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —à–∞–≥–∞
            key_metrics = []
            metrics_data = data.get("metrics", {})
            
            # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —à–∞–≥–∞
            if step_name == "main_page":
                for metric in ["lcp", "fcp", "cls"]:
                    if metric in metrics_data:
                        val = metrics_data[metric].get("mean", "‚Äî")
                        key_metrics.append(f"{metric.upper()}: {val}")
            elif step_name == "film_page":
                for metric in ["videoStartTime", "playerInitTime", "lcp"]:
                    if metric in metrics_data:
                        val = metrics_data[metric].get("mean", "‚Äî")
                        key_metrics.append(f"{metric}: {val}")
            elif step_name == "pay_page":
                for metric in ["iframeCpLoadTime"]:
                    if metric in metrics_data:
                        val = metrics_data[metric].get("mean", "‚Äî")
                        key_metrics.append(f"{metric}: {val}")
            
            # –ï—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫, –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ—Å—Ç—É–ø–Ω—ã–µ
            if not key_metrics:
                available_metrics = list(metrics_data.keys())[:3]
                for metric in available_metrics:
                    if not metric.endswith('_count'):
                        val = metrics_data[metric].get("mean", "‚Äî")
                        unit = self._get_metric_unit(metric)
                        key_metrics.append(f"{metric}: {val}{unit}")
            
            key_metrics_str = ", ".join(key_metrics) if key_metrics else "‚Äî"
            md_lines.append(f"| `{step_name}` | `{ppi_mean}` | `{ppi_stdev}` | `{key_metrics_str}` |")
        md_lines.append("")

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        md_lines.append("### üåç –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º")
        for dim, counts in summary["distribution"].items():
            md_lines.append(f"#### `{dim}`")
            md_lines.append("| –ó–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |")
            md_lines.append("|----------|------------|")
            for val, cnt in sorted(counts.items()):
                md_lines.append(f"| `{val}` | `{cnt}` |")
            md_lines.append("")

        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        problematic_metrics = self._analyze_problematic_metrics(summary)

        if problematic_metrics:
            md_lines.append("### ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
            md_lines.extend(problematic_metrics)
            md_lines.append("")
            
        if failed > 0:
            md_lines.append("### üö® –£–ø–∞–≤—à–∏–µ —Ç–µ—Å—Ç—ã (–æ—à–∏–±–∫–∏)")
            md_lines.append(f"- –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ `{failed}` –ø–∞–¥–µ–Ω–∏–π (—Å–º. —Ä–∞–∑–¥–µ–ª ¬´–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏¬ª –≤—ã—à–µ)")
            md_lines.append("")
            
        if not (problematic_metrics or failed):
            md_lines.append("### ‚úÖ –ü—Ä–æ–±–ª–µ–º –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ\n")
            
        # –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —à–∞–≥–∞–º
        md_lines.append("## üìã –î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —à–∞–≥–∞–º")
        for step_name, step_data in summary["steps"].items():
            if not step_data.get("metrics"):
                continue

            title_map = {
                "main_page": "–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞",
                "film_page": "–°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º —Ñ–∏–ª—å–º–æ–º",
                "pay_page": "–û–ø–ª–∞—Ç–∞",
                "after_payment_popup": "–ü–æ–ø–∞–ø –ø–æ—Å–ª–µ –æ–ø–ª–∞—Ç—ã",
                "after_return_without_payment": "–í–æ–∑–≤—Ä–∞—Ç –±–µ–∑ –æ–ø–ª–∞—Ç—ã"
            }
            md_lines.append(f"\n### > {title_map.get(step_name, step_name)}:")

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏: —Å–Ω–∞—á–∞–ª–∞ —á–∏—Å–ª–æ–≤—ã–µ, –ø–æ—Ç–æ–º –±—É–ª–µ–≤—ã
            metrics_data = step_data.get("metrics", {})
            for metric_name, stats in metrics_data.items():
                if not isinstance(stats, dict) or "mean" not in stats:
                    continue

                mean_val = stats["mean"]
                unit = self._get_metric_unit(metric_name)
                
                # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫–∏
                grade = config.grade_metric(mean_val, metric_name)
                icon = {"–æ—Ç–ª–∏—á–Ω–æ": "‚úÖ", "—Ö–æ—Ä–æ—à–æ": "üü¢", "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ": "üü°", "–ø–ª–æ—Ö–æ": "üî¥"}.get(grade, "‚ùì")

                # –ß–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è
                nice_name = self._get_metric_display_name(metric_name)

                # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–µ—Ç—Ä–∏–∫–∏
                if unit == "–º—Å":
                    value_str = f"{int(mean_val)} {unit}"
                    range_str = f"(min: {int(stats.get('min', 0))}, max: {int(stats.get('max', 0))})"
                elif unit == "":
                    value_str = f"{mean_val}"
                    range_str = f"(min: {stats.get('min', 0)}, max: {stats.get('max', 0)})"
                else:
                    value_str = f"{mean_val} {unit}"
                    range_str = f"(min: {stats.get('min', 0)}, max: {stats.get('max', 0)})"

                md_lines.append(f"{icon} **{nice_name}**: {value_str} {range_str} ‚Äî **{grade}**")

            # –í—ã–≤–æ–¥–∏–º –±—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏ (—Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –∏–º–µ—é—Ç –Ω–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞)
            booleans_data = step_data.get("booleans", {})
            for metric_name, stats in booleans_data.items():
                if not isinstance(stats, dict):
                    continue
                    
                true_percentage = stats.get('true_percentage', 0)
                total = stats.get('total', 0)
                
                # –í—ã–≤–æ–¥–∏–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –±—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏
                if true_percentage < 90:  # –ü–æ—Ä–æ–≥ –¥–ª—è –≤—ã–≤–æ–¥–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
                    nice_name = self._get_metric_display_name(metric_name)
                    status_icon = "üî¥" if true_percentage < 70 else "üü°"
                    md_lines.append(
                        f"{status_icon} **{nice_name}**: {true_percentage}% —É—Å–ø–µ—à–Ω–æ "
                        f"({stats.get('true_count', 0)}/{total})"
                    )
                
        md_lines.append("")
            
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))
            
        allure.attach.file(
            path,
            name="–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —à–∞–≥–∞–º",
            extension="md"
        )

    def _analyze_problematic_metrics(self, summary: dict) -> list:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–±–ª–µ–º"""
        problematic = []
        
        for step_name, data in summary["steps"].items():
            metrics_data = data.get("metrics", {})
            booleans_data = data.get("booleans", {})
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º PPI
            ppi_stats = data.get("ppi_stats", {})
            if ppi_stats.get("mean", 100) < config.TARGET_PAGE_PERFORMANCE_INDEX:
                problematic.append(f"- `{step_name}.pagePerformanceIndex`: {ppi_stats['mean']:.1f} < {config.TARGET_PAGE_PERFORMANCE_INDEX}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ—Ä–æ–≥–∞–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            for metric_name in config.METRIC_THRESHOLDS.keys():
                if metric_name in metrics_data:
                    mean_val = metrics_data[metric_name].get("mean", 0)
                    poor_threshold = config.METRIC_THRESHOLDS[metric_name][1]
                    if mean_val > poor_threshold:
                        unit = self._get_metric_unit(metric_name)
                        problematic.append(f"- `{step_name}.{metric_name}`: {mean_val:.0f}{unit} > {poor_threshold}{unit}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—É–ª–µ–≤—ã –º–µ—Ç—Ä–∏–∫–∏
            for metric_name, stats in booleans_data.items():
                if not isinstance(stats, dict):
                    continue
                    
                true_percentage = stats.get('true_percentage', 100)
                if true_percentage < 90:  # –ü–æ—Ä–æ–≥ –¥–ª—è –±—É–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
                    problematic.append(f"- `{step_name}.{metric_name}`: {true_percentage}% —É—Å–ø–µ—à–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–π < 90%")
        
        return problematic
    
    def _get_metric_unit(self, metric_name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –µ–¥–∏–Ω–∏—Ü—É –∏–∑–º–µ—Ä–µ–Ω–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏"""
        units = {
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã)
            "videoStartTime": "–º—Å",
            "playerInitTime": "–º—Å",
            "popupAppearTime": "–º—Å",
            "iframeCpLoadTime": "–º—Å",
            "lcp": "–º—Å",
            "ttfb": "–º—Å",
            "fcp": "–º—Å",
            "tbt": "–º—Å",
            "inp": "–º—Å",
            "dnsResolveTime": "–º—Å",
            "connectTime": "–º—Å",
            "rebufferDuration": "–º—Å",
            "viduPopupAppearTime": "–º—Å",
            "retryPaymentLoadTime": "–º—Å",
            
            # –ë–µ–∑—Ä–∞–∑–º–µ—Ä–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            "cls": "",
            "performance_score": "",
            "pagePerformanceIndex": "",
            "rebufferCount": "",
            
            # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            "true_percentage": "%",
        }
        return units.get(metric_name, "–º—Å")
    
    def _get_metric_display_name(self, metric_name: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ –∏–º—è –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏"""
        names = {
            "videoStartTime": "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞ –≤–∏–¥–µ–æ",
            "playerInitTime": "–ó–∞–≥—Ä—É–∑–∫–∞ –ø–ª–µ–µ—Ä–∞",
            "popupAppearTime": "–ü–æ—è–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏",
            "iframeCpLoadTime": "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ä–º—ã –æ–ø–ª–∞—Ç—ã",
            "lcp": "Largest Contentful Paint",
            "ttfb": "Time to First Byte",
            "fcp": "First Contentful Paint",
            "cls": "Cumulative Layout Shift",
            "tbt": "Total Blocking Time",
            "performance_score": "Performance Score",
            "pagePerformanceIndex": "Page Performance Index",
            "dnsResolveTime": "DNS Resolve Time",
            "connectTime": "Connect Time",
            "rebufferCount": "Rebuffer Count",
            "rebufferDuration": "Rebuffer Duration",
            "popupAvailable": "–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–æ–ø–∞–ø–∞",
            "popupClickSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∫–ª–∏–∫–∞ –ø–æ –ø–æ–ø–∞–ø—É",
            "buttonsCpAvailable": "–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–Ω–æ–ø–æ–∫ –æ–ø–ª–∞—Ç—ã",
            "buttonsClickSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –∫–ª–∏–∫–∞ –ø–æ –∫–Ω–æ–ø–∫–∞–º",
            "payFormAppear": "–ü–æ—è–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã –æ–ø–ª–∞—Ç—ã",
            "viduPopupSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ–ø–∞–ø–∞ Vidu",
            "retryPaymentSuccess": "–£—Å–ø–µ—à–Ω–æ—Å—Ç—å –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–ø–ª–∞—Ç—ã",
            "is_problematic_page": "–ü—Ä–æ–±–ª–µ–º–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞",
        }
        return names.get(metric_name, metric_name)
    
    def get_clustered_summaries(self, test_name: str, cluster_by: list = None) -> dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫–∏, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º"""
        if cluster_by is None:
            cluster_by = ["device", "throttling", "geoposition", "browser_type", "flow_branch"]
        
        cache_key = f"{test_name}_{'_'.join(sorted(cluster_by))}"
        if cache_key in self.cluster_cache:
            return self.cluster_cache[cache_key]
        
        reports = self.reports_by_test[test_name]
        if not reports:
            return {"error": f"No reports for {test_name}"}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        clusters = defaultdict(list)
        for report in reports:
            cluster_key = tuple(report.get(param, "N/A") for param in cluster_by)
            clusters[cluster_key].append(report)
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        clustered_summaries = {}
        for cluster_key, cluster_reports in clusters.items():
            cluster_name_parts = []
            for param, value in zip(cluster_by, cluster_key):
                cluster_name_parts.append(f"{param}: {value}")
            cluster_name = "; ".join(cluster_name_parts)
            
            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            temp_aggregator = MultiTestRunAggregator()
            for report in cluster_reports:
                temp_aggregator.add_report(test_name, report)
            
            clustered_summaries[cluster_name] = temp_aggregator.get_summary(test_name)
        
        self.cluster_cache[cache_key] = clustered_summaries
        return clustered_summaries
    
    def save_clustered_summaries(self, test_name: str, cluster_by: list = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã"""
        clustered_summaries = self.get_clustered_summaries(test_name, cluster_by)
        
        if "error" in clustered_summaries:
            print(f"[INFO] –ü—Ä–æ–ø—É—â–µ–Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è '{test_name}': {clustered_summaries['error']}")
            return
        
        reports_dir = Path("reports") / "clustered"
        reports_dir.mkdir(exist_ok=True, parents=True)
        
        safe_name = re.sub(r'[<>:"/\\|?*\s]', '_', test_name)[:30]
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Ç–µ—Ä
        for cluster_name, summary in clustered_summaries.items():
            safe_cluster_name = re.sub(r'[<>:"/\\|?*\s]', '_', cluster_name)[:50]
            
            json_path = reports_dir / f"CLUSTER_{safe_name}_{safe_cluster_name}.json"
            md_path = reports_dir / f"CLUSTER_{safe_name}_{safe_cluster_name}.md"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º MD —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            self._save_clustered_markdown(summary, cluster_name, md_path)
    
    def _save_clustered_markdown(self, summary: dict, cluster_name: str, path: Path):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç Markdown –æ—Ç—á–µ—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        md_lines = []
        short_cluster_name = self._shorten_cluster_name(cluster_name)
        
        md_lines.append(f"# üéØ –ö–ª–∞—Å—Ç–µ—Ä: `{cluster_name}`\n")
        md_lines.append(f"**–¢–µ—Å—Ç**: `{summary.get('test_name', 'unknown')}`")
        md_lines.append(f"**–î–æ–º–µ–Ω**: `{summary.get('domain', 'unknown')}`")
        md_lines.append(f"**–î–∞—Ç–∞**: `{time.strftime('%Y-%m-%d %H:%M:%S')}`")
        md_lines.append(f"**–ó–∞–ø—É—Å–∫–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ**: `{summary.get('total_runs', 0)}`\n")
        
        # –û—Å—Ç–∞–ª—å–Ω–∞—è —á–∞—Å—Ç—å –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞ –æ–±—ã—á–Ω–æ–º—É –æ—Ç—á–µ—Ç—É, –Ω–æ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –æ–¥–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        self._add_common_markdown_content(summary, md_lines)
        
        with open(path, "w", encoding="utf-8") as f:
            f.write("\n".join(md_lines))
    
    def _shorten_cluster_name(self, cluster_name: str) -> str:
        """–°–æ–∫—Ä–∞—â–∞–µ—Ç –∏–º—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö"""
        # –ó–∞–º–µ–Ω—è–µ–º –¥–ª–∏–Ω–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ
        """–ö–æ–º–ø–∞–∫—Ç–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ (—Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è)"""
        parts = cluster_name.split("; ")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏—è
        values = []
        for part in parts:
            if ": " in part:
                value = part.split(": ")[1]
                # –°–æ–∫—Ä–∞—â–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                short_value = {
                    "Desktop": "DT",
                    "No_throttling": "NoThrot",
                    "firefox": "FF",
                    "webkit": "WK",
                    "chromium": "CH", 
                    "Moscow": "MSK",
                    "Novosibirsk": "NSK",
                    "N/A": "NA"
                }.get(value, value[:3])
                values.append(short_value)
        
        return "-".join(values)

    def _add_common_markdown_content(self, summary: dict, md_lines: list):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –æ–±—â–µ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ Markdown (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –æ–±—ã—á–Ω—ã—Ö –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–∞—Ö)"""
        total = summary.get("total_runs", 0)
        problematic = summary.get("problematic_runs", 0)
        failed = summary.get("failed_runs", 0)
        
        md_lines.append(f"**–ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–ø—É—Å–∫–æ–≤**: `{problematic}` (`{problematic/total*100:.1f}%`)")
        md_lines.append(f"**–£–ø–∞–≤—à–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤**: `{failed}` (`{failed/total*100:.1f}%`)\n")
        
        if summary.get("errors"):
            md_lines.append("## üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏")
            md_lines.append("| –û—à–∏–±–∫–∞ | –ß–∞—Å—Ç–æ—Ç–∞ | –ü—Ä–∏–º–µ—Ä URL |")
            md_lines.append("|--------|---------|------------|")
            for error_msg, reports in sorted(summary["errors"].items(), key=lambda x: len(x[1]), reverse=True):
                count = len(reports)
                pct = count / total * 100
                example_url = reports[0].get("film_url", "N/A").split("?")[0]
                md_lines.append(f"| `{error_msg}` | `{count}` (`{pct:.1f}%`) | `{example_url}` |")
            md_lines.append("")

        # –°–≤–æ–¥–∫–∞ –ø–æ —à–∞–≥–∞–º
        md_lines.append("### üìà –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–æ —à–∞–≥–∞–º")
        md_lines.append("| –®–∞–≥ | –°—Ä–µ–¥–Ω–∏–π PPI | –ú–µ–¥–∏–∞–Ω–∞ PPI | –í–∞—Ä–∏–∞—Ü–∏—è (œÉ) |")
        md_lines.append("|-----|-------------|-------------|--------------|")

        for step_name, data in summary["steps"].items():
            ppi_stats = data.get("ppi_stats", {})
            ppi_mean = ppi_stats.get("mean", "‚Äî")
            ppi_median = ppi_stats.get("median", "‚Äî")
            ppi_stdev = ppi_stats.get("stdev", "‚Äî")
            
            md_lines.append(f"| `{step_name}` | `{ppi_mean}` | `{ppi_median}` | `{ppi_stdev}` |")
        md_lines.append("")

        # –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
        problematic_metrics = self._analyze_problematic_metrics(summary)
        if problematic_metrics:
            md_lines.append("### ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã")
            md_lines.extend(problematic_metrics)
            md_lines.append("")
            
    def create_cluster_comparison_report(self, test_name: str, cluster_by: list = None):
        """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç —Å —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        try:
            clustered_summaries = self.get_clustered_summaries(test_name, cluster_by)
            
            if "error" in clustered_summaries:
                print(f"[INFO] Cannot create cluster comparison: {clustered_summaries['error']}")
                return
            
            reports_dir = Path("reports")
            safe_name = re.sub(r'[<>:"/\\|?*\s]', '_', test_name)[:30]
            md_path = reports_dir / f"CLUSTER_COMPARISON_{safe_name}.md"
            
            md_lines = []
            md_lines.append(f"# üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: `{test_name}`\n")
            md_lines.append(f"**–î–∞—Ç–∞**: `{time.strftime('%Y-%m-%d %H:%M:%S')}`")
            md_lines.append(f"**–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏**: `{', '.join(cluster_by) if cluster_by else 'device, throttling, geo, browser_type'}`")
            md_lines.append(f"**–í—Å–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤**: `{len(clustered_summaries)}`\n")
            
            # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
            md_lines.append("## üìà –°–≤–æ–¥–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º")
            md_lines.append("| –ö–ª–∞—Å—Ç–µ—Ä | –ó–∞–ø. | –ü—Ä–æ–±–ª. | –£–ø–∞–ª–æ | PPI |")
            md_lines.append("|---------|------|---------|-------|-----|")
            
            for cluster_name, summary in clustered_summaries.items():
                total = summary.get("total_runs", 0)
                problematic = summary.get("problematic_runs", 0)
                failed = summary.get("failed_runs", 0)
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π PPI –ø–æ –≤—Å–µ–º —à–∞–≥–∞–º
                avg_ppi = self._calculate_average_ppi(summary)
                
                short_name = self._shorten_cluster_name(cluster_name)
                
                md_lines.append(
                    f"| `{short_name}` | `{total}` | `{problematic}` | `{failed}` | `{avg_ppi:.1f}` |"
                )
            
            md_lines.append("\n")
            
            # –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º - —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è
            md_lines.append("## üîç –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫")
            
            # –î–ª—è –∫–∞–∂–¥–æ–π –≤–∞–∂–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            important_metrics = [
                ("film_page", "videoStartTime", "–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∏–¥–µ–æ"),
                ("film_page", "popupAppearTime", "–ü–æ—è–≤–ª–µ–Ω–∏–µ –ø–æ–ø–∞–ø–∞"), 
                ("pay_page", "iframeCpLoadTime", "–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–æ—Ä–º—ã –æ–ø–ª–∞—Ç—ã"),
                ("film_page", "lcp", "LCP"),
                ("film_page", "pagePerformanceIndex", "PPI")
            ]
            
            for step, metric_name, display_name in important_metrics:
                md_lines.append(f"### üìä {display_name}")
                md_lines.append("| –ö–ª–∞—Å—Ç–µ—Ä | –°—Ä–µ–¥–Ω–µ–µ | –ú–µ–¥–∏–∞–Ω–∞ | Min | Max | –°—Ç–∞—Ç—É—Å |")
                md_lines.append("|---------|---------|---------|-----|-----|--------|")
                
                for cluster_name, summary in clustered_summaries.items():
                    step_data = summary.get("steps", {}).get(step, {})
                    metrics_data = step_data.get("metrics", {}).get(metric_name, {})
                    
                    if not metrics_data or "mean" not in metrics_data:
                        short_name = self._shorten_cluster_name(cluster_name)
                        md_lines.append(f"| `{short_name}` | ‚Äî | ‚Äî | ‚Äî | ‚Äî | ‚ùì |")
                        continue
                    
                    mean_val = metrics_data["mean"]
                    median_val = metrics_data["median"]
                    min_val = metrics_data["min"]
                    max_val = metrics_data["max"]
                    
                    # –û—Ü–µ–Ω–∫–∞ –º–µ—Ç—Ä–∏–∫–∏
                    grade = config.grade_metric(mean_val, metric_name)
                    icon = {"–æ—Ç–ª–∏—á–Ω–æ": "‚úÖ", "—Ö–æ—Ä–æ—à–æ": "üü¢", "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ": "üü°", "–ø–ª–æ—Ö–æ": "üî¥"}.get(grade, "‚ùì")
                    
                    # –ü–æ–ª—É—á–∞–µ–º –µ–¥–∏–Ω–∏—Ü—É –∏–∑–º–µ—Ä–µ–Ω–∏—è –î–û –µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                    unit = self._get_metric_unit(metric_name)
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è
                    if unit == "–º—Å":
                        mean_str = f"{int(mean_val)}"
                        median_str = f"{int(median_val)}"
                        min_str = f"{int(min_val)}"
                        max_str = f"{int(max_val)}"
                    else:
                        mean_str = f"{mean_val:.1f}"
                        median_str = f"{median_val:.1f}"
                        min_str = f"{min_val:.1f}"
                        max_str = f"{max_val:.1f}"
                    
                    short_name = self._shorten_cluster_name(cluster_name)
                    md_lines.append(
                        f"| `{short_name}` | `{mean_str}{unit}` | `{median_str}{unit}` | "
                        f"`{min_str}{unit}` | `{max_str}{unit}` | {icon} |"
                    )
                
                md_lines.append("")
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            md_lines.append("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑")
            analysis_results = self._analyze_clusters_statistically(clustered_summaries)
            
            if analysis_results["anomalies"]:
                md_lines.append("### ‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏")
                for anomaly in analysis_results["anomalies"]:
                    md_lines.append(f"- {anomaly}")
                md_lines.append("")
            
            if analysis_results["recommendations"]:
                md_lines.append("### üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
                for recommendation in analysis_results["recommendations"]:
                    md_lines.append(f"- {recommendation}")
                md_lines.append("")
            
            if analysis_results["best_performing"]:
                md_lines.append("### üèÜ –õ—É—á—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã")
                for best in analysis_results["best_performing"]:
                    md_lines.append(f"- {best}")
                md_lines.append("")
            
            if not any([analysis_results["anomalies"], analysis_results["recommendations"], analysis_results["best_performing"]]):
                md_lines.append("### ‚ÑπÔ∏è –û—Å–æ–±—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ\n")
            
            with open(md_path, "w", encoding="utf-8") as f:
                f.write("\n".join(md_lines))
            
            allure.attach.file(
                md_path,
                name="–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤",
                extension="md"
            )
        except Exception as e:
            print(f"[ERROR] Failed to create cluster comparison report: {e}")
            import traceback
            traceback.print_exc()
    
    def _calculate_average_ppi(self, summary: dict) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω–∏–π PPI –ø–æ –≤—Å–µ–º —à–∞–≥–∞–º"""
        try:
            ppi_values = []
            for step_name, step_data in summary.get("steps", {}).items():
                ppi_stats = step_data.get("ppi_stats", {})
                if "mean" in ppi_stats:
                    ppi_mean = ppi_stats["mean"]
                    if isinstance(ppi_mean, (int, float)) and ppi_mean > 0:
                        ppi_values.append(ppi_mean)
        
            if ppi_values:
                return statistics.mean(ppi_values)
            else:
                return 0.0
        except Exception as e:
            print(f"[ERROR] Error calculating average PPI: {e}")
            return 0.0
    
    def _find_cluster_anomalies(self, clustered_summaries: dict) -> list:
        """–ù–∞—Ö–æ–¥–∏—Ç –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        anomalies = []
        
        # –ê–Ω–∞–ª–∏–∑ PPI –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
        ppi_values = []
        for cluster_name, summary in clustered_summaries.items():
            avg_ppi = self._calculate_average_ppi(summary)
            ppi_values.append((cluster_name, avg_ppi))
        
        if ppi_values:
            avg_ppi_all = statistics.mean(ppi for _, ppi in ppi_values)
            std_ppi_all = statistics.stdev(ppi for _, ppi in ppi_values) if len(ppi_values) > 1 else 0
            
            for cluster_name, ppi in ppi_values:
                if std_ppi_all > 0 and abs(ppi - avg_ppi_all) > 2 * std_ppi_all:
                    anomalies.append(
                        f"–ö–ª–∞—Å—Ç–µ—Ä `{cluster_name}` –∏–º–µ–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–π PPI: {ppi:.1f} "
                        f"(—Å—Ä–µ–¥–Ω–µ–µ –ø–æ –≤—Å–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {avg_ppi_all:.1f})"
                    )
        
        return anomalies
    
    def _analyze_clusters_statistically(self, clustered_summaries: dict) -> dict:
        """–ü—Ä–æ–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
        results = {
            "anomalies": [],
            "recommendations": [],
            "best_performing": []
        }
        
        if len(clustered_summaries) < 2:
            results["anomalies"].append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
            return results
        
        # –ê–Ω–∞–ª–∏–∑ PPI –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
        ppi_data = []
        for cluster_name, summary in clustered_summaries.items():
            avg_ppi = self._calculate_average_ppi(summary)
            ppi_data.append((cluster_name, avg_ppi, summary.get("total_runs", 0)))
        
        if ppi_data:
            ppi_values = [ppi for _, ppi, _ in ppi_data]
            avg_ppi_all = statistics.mean(ppi_values)
            
            if len(ppi_values) > 1:
                std_ppi_all = statistics.stdev(ppi_values)
                
                # –ù–∞—Ö–æ–¥–∏–º –∞–Ω–æ–º–∞–ª–∏–∏ (–±–æ–ª–µ–µ 2 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π)
                for cluster_name, ppi, runs in ppi_data:
                    if std_ppi_all > 0 and abs(ppi - avg_ppi_all) > 2 * std_ppi_all:
                        short_name = self._shorten_cluster_name(cluster_name)
                        results["anomalies"].append(
                            f"–ö–ª–∞—Å—Ç–µ—Ä `{short_name}` –∏–º–µ–µ—Ç –∞–Ω–æ–º–∞–ª—å–Ω—ã–π PPI: {ppi:.1f} "
                            f"(—Å—Ä–µ–¥–Ω–µ–µ: {avg_ppi_all:.1f} ¬± {std_ppi_all:.1f})"
                        )
            
            # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–µ –∏ —Ö—É–¥—à–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
            best_ppi = max(ppi_values)
            worst_ppi = min(ppi_values)
            
            for cluster_name, ppi, runs in ppi_data:
                short_name = self._shorten_cluster_name(cluster_name)
                if ppi == best_ppi and runs >= 3:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                    results["best_performing"].append(
                        f"`{short_name}` - –ª—É—á—à–∏–π PPI: {ppi:.1f} (–∑–∞–ø—É—Å–∫–æ–≤: {runs})"
                    )
                elif ppi == worst_ppi and runs >= 3:
                    results["anomalies"].append(
                        f"`{short_name}` - —Ö—É–¥—à–∏–π PPI: {ppi:.1f} (–∑–∞–ø—É—Å–∫–æ–≤: {runs})"
                    )
        
        # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ
        video_times = []
        for cluster_name, summary in clustered_summaries.items():
            film_metrics = summary.get("steps", {}).get("film_page", {})
            video_time = film_metrics.get("metrics", {}).get("videoStartTime", {}).get("mean", 0)
            if video_time > 0:
                video_times.append((cluster_name, video_time))
        
        if video_times:
            avg_video_time = statistics.mean(time for _, time in video_times)
            best_video = min(video_times, key=lambda x: x[1])
            worst_video = max(video_times, key=lambda x: x[1])
            
            short_best = self._shorten_cluster_name(best_video[0])
            short_worst = self._shorten_cluster_name(worst_video[0])
            
            results["recommendations"].append(
                f"–õ—É—á—à–µ–µ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–∏–¥–µ–æ: `{short_best}` ({best_video[1]:.0f}–º—Å), "
                f"—Ö—É–¥—à–µ–µ: `{short_worst}` ({worst_video[1]:.0f}–º—Å)"
            )
        
        # –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success_rates = []
        for cluster_name, summary in clustered_summaries.items():
            total = summary.get("total_runs", 0)
            failed = summary.get("failed_runs", 0)
            if total > 0:
                success_rate = (total - failed) / total * 100
                success_rates.append((cluster_name, success_rate, total))
        
        if success_rates:
            worst_success = min(success_rates, key=lambda x: x[1])
            if worst_success[1] < 80 and worst_success[2] >= 3:  # –ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
                short_name = self._shorten_cluster_name(worst_success[0])
                results["anomalies"].append(
                    f"–ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ `{short_name}`: {worst_success[1]:.1f}% "
                    f"({worst_success[2]-failed} –∏–∑ {worst_success[2]})"
                )
        
        return results